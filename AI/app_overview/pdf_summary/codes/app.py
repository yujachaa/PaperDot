import warnings
import os
import asyncio
from pdf_summary.codes.crawler import download_pdf  # embedding.pyê°€ì„œ ê°€ì ¸ì˜´
from contextlib import asynccontextmanager
import uvicorn
import pickle
from dotenv import load_dotenv
from fastapi import FastAPI, Query, HTTPException
from pydantic import BaseModel
import numpy as np
import re
import urllib
import urllib.parse
from fastapi.middleware.cors import CORSMiddleware
from asyncio import Queue

from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables import chain
from langchain_core.documents import Document
from langchain_text_splitters import MarkdownHeaderTextSplitter
from langchain.prompts import PromptTemplate
from elasticsearch import Elasticsearch
from langchain import hub
# from sklearn.cluster import KMeans
# from sklearn.manifold import TSNE
import openai
from langchain_core.runnables import chain
import pymupdf4llm


current_dir = os.path.dirname(os.path.abspath(__file__))
env_path = os.path.join(current_dir, "../../config/.env")

# Define paths
MAPPING_PICKLE_FILE = os.path.join(current_dir, "../models/doc_id_index_mapping.pkl")
PAPER_STORAGE_PATH = os.path.join(current_dir, "../datas/")

load_dotenv(dotenv_path=env_path)

openai.api_key = os.getenv("OPENAI_API_KEY")

# Elasticsearch ì„¤ì •
ES_HOST = os.getenv('ES_HOST')  # ì˜ˆ: 'localhost' ë˜ëŠ” 'your-ec2-public-dns'
ES_PORT = os.getenv('ES_PORT')  # ê¸°ë³¸ í¬íŠ¸; ë‹¤ë¥¼ ê²½ìš° ë³€ê²½
ES_USER = os.getenv('ES_USER')  # ì¸ì¦ì´ í•„ìš”í•œ ê²½ìš°
ES_PASSWORD = os.getenv('ES_PASSWORD')  # ì¸ì¦ì´ í•„ìš”í•œ ê²½ìš°
ES_APIKEY = os.getenv('ES_APIKEY')
INDEX_NAME = 'papers'

mapper = None
reverse_mapper = None
llm = None
mapper = None

# CORS ì„¤ì • ì¶”ê°€
origins = [
    "http://localhost:5173",
    "https://localhost:5173",  # ì˜ˆë¥¼ ë“¤ì–´ ë¦¬ì•¡íŠ¸ ë¡œì»¬ ì„œë²„
    "https://j11b208.p.ssafy.io",  # ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ” ë„ë©”ì¸ ì¶”ê°€
]

headers_to_split_on = [
    (
        "#",
        "Header 1",
    ),
    (
        "##",
        "Header 2",
    ),
    (
        "###",
        "Header 3",
    ),
]

prompt_template = """
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ì½ê³  ë˜ë„ë¡ í•œê¸€ë¡œ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”. ìš”ì•½ ì‹œì—ëŠ” markdown íƒœê·¸ë¥¼ ì ê·¹ í™œìš©í•´ì£¼ì„¸ìš”. '#' í—¤ë”ê°€ ìˆë‹¤ë©´ ê° ë¬¸ë‹¨ì— ì–´ìš¸ë¦¬ëŠ” ì´ëª¨ì§€ë¥¼ í—¤ë”ì— í¬í•¨í•´ ê¾¸ë©°ì£¼ì„¸ìš”.

"{text}"

ìš”ì•½:
"""

PROMPT = PromptTemplate(template=prompt_template, input_variables=["text"])

queue = Queue()

async def download_worker():
    while True:
        paper_id, paper_path, reverse_mapper = await queue.get()
        try:
            download_pdf(reverse_mapper[int(paper_id)], paper_path)
        except Exception as e:
            print(f"ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            queue.task_done()

def load_mapping_pickle_data(pickle_file):
    """
    í”¼í´ íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    if not os.path.exists(pickle_file):
        raise FileNotFoundError(f"í”¼í´ íŒŒì¼ {pickle_file}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    with open(pickle_file, 'rb') as f:
        data = pickle.load(f)
    return data

def create_internal_links(markdown_text):
    # ì •ê·œ í‘œí˜„ì‹ íŒ¨í„´: ì¤„ ì‹œì‘ì—ì„œ í•˜ë‚˜ ì´ìƒì˜ '#' ë’¤ì— ê³µë°±ê³¼ í…ìŠ¤íŠ¸ê°€ ì˜¤ëŠ” íŒ¨í„´
    header_pattern = re.compile(r'^(#)\s+(.*)', re.MULTILINE)
    
    # ëª¨ë“  í—¤ë” ì°¾ê¸°
    headers = header_pattern.findall(markdown_text)
    
    links = []
    for i, header in enumerate(headers, start=1):
        hashes, header_text = header
        # ê³µë°±ì„ '-'ë¡œ ëŒ€ì²´í•˜ê³  ì†Œë¬¸ìë¡œ ë³€í™˜ (GitHub ìŠ¤íƒ€ì¼ ì•µì»¤ ê¸°ì¤€)
        anchor = re.sub(r'\s+', '-', header_text.strip()).lower()
        anchor = re.sub(r'[^\w\-\u2600-\u27BF\u1F300-\u1FAFF\u1F900-\u1F9FF\u1F600-\u1F64F]', '', anchor)

        # ë‚´ë¶€ ë§í¬ í˜•ì‹ ìƒì„±
        link = f"[{i}. {header_text}](#{anchor})"
        links.append(link)
    links = "<br>".join(links)
    return links

async def get_pdf(paper_path, paper_id, reverse_mapper):
    if not os.path.exists(paper_path):
        await queue.put((paper_id, paper_path, reverse_mapper))
        await queue.join()
    with open(paper_path, "rb") as f:
        pdf_document = f.read()
    return pdf_document

# Elasticsearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±
def create_es_client(host=ES_HOST, port=ES_PORT, user=ES_USER, password=ES_PASSWORD):
    """
    Elasticsearch í´ë¼ì´ì–¸íŠ¸ì— ì—°ê²°í•©ë‹ˆë‹¤.
    """
    if user and password:
        es = Elasticsearch(
            f"http://{host}:{port}",
            basic_auth=(user, password),
            request_timeout=60,
        )
    else:
        es = Elasticsearch(
            f"http://{host}:{port}",
            request_timeout=60,
        )
    # ì—°ê²° í™•ì¸
    if not es.ping():
        raise ValueError("Elasticsearch ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    print("Elasticsearchì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return es

class QueryResponse(BaseModel):
    answer: str
    model: int

# FastAPI lifespan event handler
@asynccontextmanager
async def lifespan(app: FastAPI):
    global mapper, reverse_mapper, llm
    # ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì‹¤í–‰ë  ì´ˆê¸°í™” ë¡œì§
    print("Initializing embedding system...")
    mapper = load_mapping_pickle_data(MAPPING_PICKLE_FILE)
    reverse_mapper = {v: k for k, v in mapper.items()}

    llm = ChatOpenAI(
        model_name="gpt-4o-mini",
        streaming=True,
        temperature=0,
    )

    # ë‹¤ìš´ë¡œë“œ ì›Œì»¤ íƒœìŠ¤í¬ ì‹œì‘
    app.download_worker_task = asyncio.create_task(download_worker())

    # lifespanì— ì§„ì… (ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ì¤‘)
    yield

    # ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œ ì‹¤í–‰ë  ì •ë¦¬ ì‘ì—…
    print("Shutting down...")
    app.download_worker_task.cancel()

# FastAPI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±, lifespan ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì‚¬ìš©
app = FastAPI(lifespan=lifespan)
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,  # í—ˆìš©í•  ë„ë©”ì¸
    allow_credentials=True,
    allow_methods=["*"],  # ëª¨ë“  ë©”ì„œë“œ í—ˆìš© (GET, POST ë“±)
    allow_headers=["*"],  # ëª¨ë“  í—¤ë” í—ˆìš©
)

async def agent_pipeline(paper_path, paper_id):
    global mapper, reverse_mapper
    pdf_document = await get_pdf(paper_path, paper_id, reverse_mapper)

    markdown_document = pymupdf4llm.to_markdown(paper_path)

    markdown_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=headers_to_split_on,
        strip_headers=False, # í—¤ë” ì œê±° off
    )

    md_header_splits = markdown_splitter.split_text(markdown_document)

    md_header_splits = [section.page_content for section in md_header_splits]

    llm = ChatOpenAI(
        temperature=0,
        model_name="gpt-4o-mini",
        streaming=True,
    )

    map_chain = PROMPT | llm | StrOutputParser()

    doc_summaries = map_chain.batch(md_header_splits)

    len(doc_summaries)

    doc_summaries = '\n\n'.join(doc_summaries)

    internal_links = create_internal_links(doc_summaries)

    toc_markdown = f"# ëª©ì°¨\n\n{internal_links}\n\n"
    final_markdown = toc_markdown + '\n --- \n' + doc_summaries

    return final_markdown

@app.get("/summary")
async def summary_paper(paper_id: str = Query(..., description="Paper ID to search"), gen: bool = Query(..., description="RE:generate flag")):
    """
    ìš”ì•½ API ì—”ë“œí¬ì¸íŠ¸ë¡œ, GET ìš”ì²­ìœ¼ë¡œ ì „ë‹¬ëœ idì— ëŒ€í•´ ìš”ì•½ëœ markdown ë°˜í™˜.
    """
    global mapper, reverse_mapper
    es = create_es_client()

    res = es.get(index=INDEX_NAME, id=paper_id, ignore=404)

    # esì— ìˆë‹¤ë©´
    if res['found']:
        doc = res['_source']
        # overview í•„ë“œê°€ ë¹„ì–´ ìˆëŠ”ì§€ í™•ì¸
        if 'overview' in doc and doc['overview'] and not gen:
            # ì´ë¯¸ ìš”ì•½ëœ ë‚´ìš©ì´ ìˆë‹¤ë©´ ê·¸ ë‚´ìš©ì„ ë°˜í™˜
            return {"results": doc['overview'], "model": 0}

        # esì— ì—†ë‹¤ë©´ pdf ë¡œë”
        else:
            paper_path = f"{PAPER_STORAGE_PATH}{paper_id}.pdf"
            results = await agent_pipeline(paper_path, paper_id)

            es.update(index=INDEX_NAME, id=paper_id, body={"doc": {"overview": results}})

            return {"results": results, "model": 1}

    # es ì— ì‚½ì…

    # es ì¢…ë£Œ

    results = "\n\n ## ğŸ™ ì¬ìš”ì•½ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”. ğŸ™"
    return {"results": results, "mdoel": 0}

def main():
    """
    í¸ì˜ì„±ì„ ìœ„í•œ main í•¨ìˆ˜. uvicornì„ ì‚¬ìš©í•´ FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹¤í–‰.
    """
    uvicorn.run("app:app", host="0.0.0.0", port=3333, reload=True)

if __name__ == "__main__":
    try:
        # ê¸°ì¡´ ì½”ë“œ ì‹¤í–‰
        main()  # ì£¼ ì‹¤í–‰ ì½”ë“œ
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()